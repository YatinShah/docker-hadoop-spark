version: "3"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9010:9000
    volumes:
      - ./data/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    networks:
      - big3
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 3G
        reservations:
          cpus: '0.1'
          memory: 1G
          devices:
            - capabilities: [gpu]
              count: 1                

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - ./data/datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    ports:
      - "9864:9864"
    env_file:
      - ./hadoop.env
    networks:
      - big3
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 3G
        reservations:
          cpus: '0.1'
          memory: 1G
          devices:
            - capabilities: [gpu]
              count: 1                

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./hadoop.env
    networks:
      - big3
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 1G
        reservations:
          cpus: '0.05'
          memory: 500M

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env
    networks:
      - big3
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 3G
        reservations:
          cpus: '0.1'
          memory: 1G

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - ./data/historysrvr:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env
    networks:
      - big3
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 1G
        reservations:
          cpus: '0.08'
          memory: 500M

  spark-master:
    image: bde2020/spark-master:3.0.0-hadoop3.2
    container_name: spark-master
    depends_on:
      - namenode
      - datanode
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - big3
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 3G
        reservations:
          cpus: '0.1'
          memory: 1G

  spark-worker-1:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - big3
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 3G
        reservations:
          cpus: '0.1'
          memory: 1G

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    depends_on:
      - namenode
      - datanode
    env_file:
      - ./hadoop-hive.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
    ports:
      - "10000:10000"
    networks:
      - big3
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 3G
        reservations:
          cpus: '0.1'
          memory: 1G

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode:9864 hive-metastore-postgresql:5432"
    ports:
      - "9083:9083"
    networks:
      - big3
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 3G
        reservations:
          cpus: '0.1'
          memory: 1G

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql
    networks:
      - big3
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 3G
        reservations:
          cpus: '0.1'
          memory: 1G

  presto-coordinator:
    image: shawnzhu/prestodb:0.181
    container_name: presto-coordinator
    ports:
      - "8089:8089"
    networks:
      - big3
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 1G
        reservations:
          cpus: '0.05'
          memory: 500M

  firefox:
    image: jlesage/firefox
    hostname: firefox
    container_name: firefox
    ports:
      - 5800:5800
    networks:
      - big3
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 1G
        reservations:
          cpus: '0.05'
          memory: 500M
          devices:
            - capabilities: [gpu]
              count: 1                

  zookeeper:
    image: 'wurstmeister/zookeeper'
    hostname: 'zookeeper'
    container_name: 'zookeeper'
    ports:
      - '2181:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - big3
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 1G
        reservations:
          cpus: '0.05'
          memory: 500M

  kafka1:
    image: 'confluentinc/cp-kafka:7.3.2'
    hostname: 'kafka1'
    container_name: 'kafka1'
    ports:
      - '9092:9092'
      - '19092:19092'
      - '29092:29092'
    environment:
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://kafka1:29092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:19092'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: 'kafka.controller=DEBUG,kafka.producer.async.DefaultEventHandler=DEBUG,state.change.logger=DEBUG'
      KAFKA_AUTHORIZER_CLASS_NAME: 'kafka.security.authorizer.AclAuthorizer'
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: 'true'
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    depends_on:
      - 'zookeeper'
    networks:
      - big3
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 1G
        reservations:
          cpus: '0.05'
          memory: 500M

networks:
  big3:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.host_binding_ipv4: "127.0.0.1"    

#volumes:
  # hadoop_namenode:
  # hadoop_datanode:
  # hadoop_historyserver:

# Urls:
#   Firefox: http://localhost:5800, OR http://ubuntu:5800 (from a remote machine)
#     
#     then in the url type these urls to access the websites, they all run except the hive-server.
#     Namenode: http://namenode:9870/dfshealth.html#tab-overview
#     History server: http://historyserver:8188/applicationhistory
#     Datanode: http://datanode:9864/
#     Nodemanager: http://nodemanager:8042/node
#     Resource manager: http://resourcemanager:8088/
#     Spark master: http://spark-master:8080/
#     Spark worker: http://spark-worker-1:8081/
#     Hive: http://hive-server:10000
