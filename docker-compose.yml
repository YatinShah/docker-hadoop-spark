# version: "3"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9010:9000
    volumes:
      - ./data/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    networks:
      - big3
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.2'
    #       memory: 3G
    #     reservations:
    #       cpus: '0.1'
    #       memory: 1G
    #       devices:
    #         - capabilities: [gpu]
    #           count: 1                

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - ./data/datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    ports:
      - "9864:9864"
    env_file:
      - ./hadoop.env
    networks:
      - big3
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.2'
    #       memory: 3G
    #     reservations:
    #       cpus: '0.1'
    #       memory: 1G
    #       devices:
    #         - capabilities: [gpu]
    #           count: 1                

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./hadoop.env
    networks:
      - big3
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.1'
    #       memory: 1G
    #     reservations:
    #       cpus: '0.05'
    #       memory: 500M

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env
    networks:
      - big3
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.2'
    #       memory: 3G
    #     reservations:
    #       cpus: '0.1'
    #       memory: 1G

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - ./data/historysrvr:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env
    networks:
      - big3
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.1'
    #       memory: 1G
    #     reservations:
    #       cpus: '0.08'
    #       memory: 500M

  spark-master:
    image: bde2020/spark-master:3.0.0-hadoop3.2
    container_name: spark-master
    depends_on:
      - namenode
      - datanode
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - big3
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.2'
    #       memory: 3G
    #     reservations:
    #       cpus: '0.1'
    #       memory: 1G

  spark-worker-1:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - big3
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.2'
    #       memory: 3G
    #     reservations:
    #       cpus: '0.1'
    #       memory: 1G

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    depends_on:
      - namenode
      - datanode
    env_file:
      - ./hadoop-hive.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
    ports:
      - "10000:10000"
    networks:
      - big3
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.2'
    #       memory: 3G
    #     reservations:
    #       cpus: '0.1'
    #       memory: 1G

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode:9864 hive-metastore-postgresql:5432"
    ports:
      - "9083:9083"
    networks:
      - big3
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.2'
    #       memory: 3G
    #     reservations:
    #       cpus: '0.1'
    #       memory: 1G

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql
    networks:
      - big3
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.2'
    #       memory: 3G
    #     reservations:
    #       cpus: '0.1'
    #       memory: 1G

  presto-coordinator:
    image: shawnzhu/prestodb:0.181
    container_name: presto-coordinator
    ports:
      - "8089:8089"
    networks:
      - big3
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.1'
    #       memory: 1G
    #     reservations:
    #       cpus: '0.05'
    #       memory: 500M

  firefox:
    image: jlesage/firefox
    hostname: firefox
    container_name: firefox
    ports:
      - 5800:5800
    networks:
      - big3
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.1'
    #       memory: 1G
    #     reservations:
    #       cpus: '0.05'
    #       memory: 500M
    #       devices:
    #         - capabilities: [gpu]
    #           count: 1                

  zookeeper:
    image: 'wurstmeister/zookeeper'
    hostname: 'zookeeper'
    container_name: 'zookeeper'
    ports:
      - '2181:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - big3
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.1'
    #       memory: 1G
    #     reservations:
    #       cpus: '0.05'
    #       memory: 500M

  kafka1:
    image: 'confluentinc/cp-kafka:7.3.2'
    hostname: 'kafka1'
    container_name: 'kafka1'
    ports:
      - '9092:9092'
      - '19092:19092'
      - '29092:29092'
    environment:
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://kafka1:29092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:19092'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: 'kafka.controller=DEBUG,kafka.producer.async.DefaultEventHandler=DEBUG,state.change.logger=DEBUG'
      KAFKA_AUTHORIZER_CLASS_NAME: 'kafka.security.authorizer.AclAuthorizer'
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: 'true'
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    depends_on:
      - 'zookeeper'
    networks:
      - big3
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.1'
    #       memory: 1G
    #     reservations:
    #       cpus: '0.05'
    #       memory: 500M

  kafka-connect0:
    image: confluentinc/cp-kafka-connect:6.0.1
    container_name: 'kafka-connect0'
    ports:
      - 8083:8083
    depends_on:
      - kafka1
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka1:29092
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: _connect_configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_TOPIC: _connect_offset
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: _connect_status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      # CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schemaregistry0:8085
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      # CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schemaregistry0:8085
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect0
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
    networks:
      - big3

  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8099:8080
    depends_on:
      - zookeeper
      - kafka1
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka1:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
      KAFKA_CLUSTERS_0_JMXPORT: 9998
      # KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schemaregistry1:8085
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: first
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect0:8083
    networks:
      - big3

  kafka-init-topics:
    image: confluentinc/cp-kafka:5.3.1
    container_name: init-topics
    depends_on:
      - kafka1
    command: "bash -c 'echo Waiting for Kafka to be ready... && \
               cub kafka-ready -b kafka1:29092 1 30 && \
               kafka-topics --create --topic aggNews --partitions 2 --replication-factor 1 --if-not-exists --zookeeper zookeeper:2181 && \
               kafka-topics --create --topic incomingNews --partitions 2 --replication-factor 1 --if-not-exists --zookeeper zookeeper:2181' "
    networks:
      - big3


networks:
  big3:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.host_binding_ipv4: "127.0.0.1"    

#volumes:
  # hadoop_namenode:
  # hadoop_datanode:
  # hadoop_historyserver:

# Urls:
#   Firefox: http://localhost:5800, OR http://ubuntu:5800 (from a remote machine)
#     
#     then in the url type these urls to access the websites, they all run except the hive-server.
#     Namenode: http://namenode:9870/dfshealth.html#tab-overview
#     History server: http://historyserver:8188/applicationhistory
#     Datanode: http://datanode:9864/
#     Nodemanager: http://nodemanager:8042/node
#     Resource manager: http://resourcemanager:8088/
#     Spark master: http://spark-master:8080/
#     Spark worker: http://spark-worker-1:8081/
#     Hive: http://hive-server:10000
